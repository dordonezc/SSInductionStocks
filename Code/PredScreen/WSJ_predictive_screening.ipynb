{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863c86e4",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c1a1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import datetime\n",
    "\n",
    "import beautifultools as bt\n",
    "import qgrid\n",
    "from pandas.core.common import flatten\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import scipy.stats\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "random.seed(3)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from RandomWalk import random_walk\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append('/home/ec2-user/SageMaker/nltk_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f36824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "wsj = pd.read_csv('wsj_full1.csv') # wsj dataset\n",
    "sp100 = pd.read_csv('..//data/LogReturnData.csv')\n",
    "\n",
    "# select the relevant topics\n",
    "tp_li = [0, 2, 3, 8, 9, 14, 16, 17, 19, 20, 21, 24]\n",
    "wsj_selected = wsj[wsj['Topic_Num'].isin(tp_li)] \n",
    "\n",
    "# only the log returns of S&P100 is selected\n",
    "oex = sp100[['Date', '^OEX']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2113999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.conda/envs/sample_env/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/SageMaker/.conda/envs/sample_env/lib/python3.7/site-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "# label the return with positive & negative, 1 refers to positive log return, 0 refers to negative log return\n",
    "oex['direction'] = 1\n",
    "oex.loc[oex[oex['^OEX'] < 0].index, 'direction'] = -1\n",
    "\n",
    "# drop NaN value\n",
    "oex = oex.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04abac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj1 = wsj_selected.copy() # make a copy of wsj_selected\n",
    "\n",
    "# select relevant columns, polarity calculated with Mcdonald dict for future comparison\n",
    "wsj1 = wsj1[['Title', 'Text', 'Date']]\n",
    "\n",
    "# convert the date to datetime \n",
    "wsj1['Date'] = wsj1['Date'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").date())\n",
    "oex['Date'] = oex['Date'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").date())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcfbd31",
   "metadata": {},
   "source": [
    "## 2. Text Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f911da5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load stopping words\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "all_stopwords = sp.Defaults.stop_words # list of stop words\n",
    "\n",
    "# remove 'up', 'down' from stop words\n",
    "all_stopwords.remove('up')\n",
    "all_stopwords.remove('down')\n",
    "\n",
    "## Change Numbers info for placeholder keep signs \n",
    "txt_new = []\n",
    "reg = re.compile(r\"([\\\\+\\\\-])?[0-9]+[0-9\\\\.]*\")\n",
    "for lines in wsj1[\"Text\"].values:\n",
    "    txt_new.append(reg.sub(\" \\\\1NUM\", lines))\n",
    "\n",
    "## Define punctuation to replace (Exclude +, -, and %)\n",
    "new_punct = string.punctuation + \"“”’\"\n",
    "for symb in [\"%\", \"+\", \"-\", \"&\"]:\n",
    "    new_punct = new_punct.replace(symb, \"\")\n",
    "\n",
    "## String list\n",
    "txt_corp = []\n",
    "for doc in txt_new:\n",
    "    ## Change everything to lowercase and exclude string that are only punctuations and stop words\n",
    "    aux = [elem.lower() for elem in doc.split() if elem not in set(new_punct)]\n",
    "    nstop = [wo for wo in aux if wo not in all_stopwords]\n",
    "    txt_corp.append(nstop)\n",
    "\n",
    "## Remove strings that only have punctuation signs\n",
    "exclude = [\"\"]\n",
    "txt_end = []\n",
    "for doc in txt_corp:\n",
    "    new_list = [elem.translate(str.maketrans('', '', new_punct)) for elem in doc]\n",
    "    txt_end.append([elem for elem in new_list if elem not in exclude])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e2d8b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rate for 30-Year Mortgage Falls to Lowest on R...</td>\n",
       "      <td>\\nIn a year of financial firsts, this one stan...</td>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>[year, financial, firsts, stands, out, mortgag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dollar's Surge Is Hurdle for Shares</td>\n",
       "      <td>\\nInvestors worried about the impact of the co...</td>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>[investors, worried, impact, coronavirus, worl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Banking &amp;amp; Finance: Clayton Dubilier Fund H...</td>\n",
       "      <td>\\nClayton Dubilier &amp; Rice could collect about ...</td>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>[clayton, dubilier, &amp;, rice, collect, num, bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S&amp;amp;P Edges Higher to Record Close</td>\n",
       "      <td>\\nThe S&amp;P 500 and Nasdaq Composite notched rec...</td>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>[s&amp;p, num, nasdaq, composite, notched, records...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Campbell And Loeb Are Close To a Deal</td>\n",
       "      <td>Campbell Soup Co. is nearing a settlement with...</td>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>[campbell, soup, co, nearing, settlement, inve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "1  Rate for 30-Year Mortgage Falls to Lowest on R...   \n",
       "5                Dollar's Surge Is Hurdle for Shares   \n",
       "6  Banking &amp; Finance: Clayton Dubilier Fund H...   \n",
       "7               S&amp;P Edges Higher to Record Close   \n",
       "9              Campbell And Loeb Are Close To a Deal   \n",
       "\n",
       "                                                Text        Date  \\\n",
       "1  \\nIn a year of financial firsts, this one stan...  2020-07-17   \n",
       "5  \\nInvestors worried about the impact of the co...  2020-02-10   \n",
       "6  \\nClayton Dubilier & Rice could collect about ...  2020-10-22   \n",
       "7  \\nThe S&P 500 and Nasdaq Composite notched rec...  2020-08-26   \n",
       "9  Campbell Soup Co. is nearing a settlement with...  2018-11-26   \n",
       "\n",
       "                                              corpus  \n",
       "1  [year, financial, firsts, stands, out, mortgag...  \n",
       "5  [investors, worried, impact, coronavirus, worl...  \n",
       "6  [clayton, dubilier, &, rice, collect, num, bil...  \n",
       "7  [s&p, num, nasdaq, composite, notched, records...  \n",
       "9  [campbell, soup, co, nearing, settlement, inve...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj1['corpus'] = txt_end\n",
    "wsj1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f071ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label article with direction of sp100\n",
    "wsj1['logDate'] = wsj1['Date'].apply(lambda x: x + datetime.timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b183303",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj1.to_csv('cleaned_corpus.csv') # save the cleaned corpus to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd66084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = wsj1.set_index('logDate').join(oex.set_index('Date')) # with lag\n",
    "df2 = wsj1.set_index('Date').join(oex.set_index('Date')) # without lag\n",
    "\n",
    "# remove NaN value\n",
    "df1 = df1.dropna()\n",
    "df2 = df2.dropna()\n",
    "# reset the index\n",
    "df1 = df1.reset_index()\n",
    "df2 = df2.reset_index()\n",
    "\n",
    "df1 = df1.drop('Date', 1) # drop the date column\n",
    "df2 = df2.drop('logDate', 1) # drop the date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06fff7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column\n",
    "df1.columns = ['date', 'Title', 'Text', 'corpus', '^OEX', 'direction']\n",
    "df2.columns = ['date', 'Title', 'Text', 'corpus', '^OEX', 'direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fad3a57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    472.000000\n",
       "mean      30.023305\n",
       "std        6.244954\n",
       "min       13.000000\n",
       "25%       26.000000\n",
       "50%       30.000000\n",
       "75%       34.000000\n",
       "max       56.000000\n",
       "Name: Title, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.groupby('date')['Title'].count().describe() # number of articles everyday, index column refers to date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937fe2b",
   "metadata": {},
   "source": [
    "## 3. Predictive Screening to get the seed words\n",
    "### 3.1 seed words with lag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd4e8f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training & testing dataset to avoid data learkage\n",
    "train_lag = df1.groupby('date').apply(lambda x: x.sample(frac=0.1))\n",
    "train_ind = [index[1] for index in train_lag.index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20b5b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['data'] = 'test'\n",
    "df1.loc[train_ind, 'data'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4fb9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a datadframe that contains the positive/negative words\n",
    "def create_df(i, train, df):\n",
    "    words = df[(df['direction'] == i) & (df['data'] == train)].corpus.tolist()\n",
    "    words = sum(words, []) # flattern list of lists\n",
    "    word_dict = dict(Counter(words)) # word count\n",
    "    count_df = pd.DataFrame.from_dict(word_dict, orient = 'index') # convert dict to df\n",
    "\n",
    "    count_df = count_df.reset_index()\n",
    "    count_df.columns = ['word', 'freq']\n",
    "    return count_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d32da1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training dataset\n",
    "pos_word = create_df(1, 'train', df1)\n",
    "neg_word = create_df(-1, 'train', df1)\n",
    "\n",
    "neg_word.columns = ['word', 'neg_freq']\n",
    "# pos_word.columns = ['word', 'neg_freq']\n",
    "word = pos_word.set_index('word').join(neg_word.set_index('word')) # join pos_word, neg_word dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b78b8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, num):\n",
    "    # replace NaN with 0\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # reset index\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # select only the word with frequency higher than 50\n",
    "    df['total_freq'] = df['freq'] + df['neg_freq']\n",
    "    df = df[df['total_freq'] >= num]\n",
    "    \n",
    "    df['pos_prob'] = df['freq']/(df['freq'] + df['neg_freq']) # prob that specific word appear in a positive article\n",
    "    df['neg_prob'] = 1 - df['pos_prob']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6b5178f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>neg_freq</th>\n",
       "      <th>total_freq</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>neg_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3677</th>\n",
       "      <td>etfs</td>\n",
       "      <td>103</td>\n",
       "      <td>5.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.046296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>sears</td>\n",
       "      <td>60</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>etf</td>\n",
       "      <td>79</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.048193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>coffee</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>pipeline</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  freq  neg_freq  total_freq  pos_prob  neg_prob\n",
       "3677      etfs   103       5.0       108.0  0.953704  0.046296\n",
       "1285     sears    60       3.0        63.0  0.952381  0.047619\n",
       "1544       etf    79       4.0        83.0  0.951807  0.048193\n",
       "5918    coffee    51       3.0        54.0  0.944444  0.055556\n",
       "194   pipeline    51       3.0        54.0  0.944444  0.055556"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prob = filter_df(word, 50).sort_values(by = ['pos_prob'], ascending=False) \n",
    "df_prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293b6bb",
   "metadata": {},
   "source": [
    "### Determine the threshold with binomial Confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d1dc4519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence interval of positive seed words:  (0.5473500022088275, 0.5924352982371425)\n",
      "confidence interval of negative seed words:  (0.4075647017628575, 0.4526499977911725)\n"
     ]
    }
   ],
   "source": [
    "################# to be confirmed #################\n",
    "import statsmodels.stats.proportion as smp\n",
    "\n",
    "thres = 0.56\n",
    "pos = df_prob[df_prob['pos_prob'] >= thres]\n",
    "count = len(pos)\n",
    "num = len(df_prob)\n",
    "\n",
    "print('confidence interval of positive seed words: ', smp.proportion_confint (count, num, alpha=0.05, method='wilson'))\n",
    "print('confidence interval of negative seed words: ', smp.proportion_confint (num - count, num, alpha=0.05, method='wilson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "905df423",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## to be confirmed ###############\n",
    "df_prob['polar'] = 'positive'\n",
    "df_prob.loc[df_prob[df_prob['pos_prob'] < 0.56].index, 'polar'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2fd7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob.to_csv('seed_words_lag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1cd9c8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>neg_freq</th>\n",
       "      <th>total_freq</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>neg_prob</th>\n",
       "      <th>polar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3677</th>\n",
       "      <td>etfs</td>\n",
       "      <td>103</td>\n",
       "      <td>5.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>sears</td>\n",
       "      <td>60</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>etf</td>\n",
       "      <td>79</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>coffee</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>pipeline</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  freq  neg_freq  total_freq  pos_prob  neg_prob     polar\n",
       "3677      etfs   103       5.0       108.0  0.953704  0.046296  positive\n",
       "1285     sears    60       3.0        63.0  0.952381  0.047619  positive\n",
       "1544       etf    79       4.0        83.0  0.951807  0.048193  positive\n",
       "5918    coffee    51       3.0        54.0  0.944444  0.055556  positive\n",
       "194   pipeline    51       3.0        54.0  0.944444  0.055556  positive"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637380dc",
   "metadata": {},
   "source": [
    "### 3.2 seed words without lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34d5a52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>neg_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>detroit</th>\n",
       "      <td>17</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--</th>\n",
       "      <td>1385</td>\n",
       "      <td>1063.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>1314</td>\n",
       "      <td>1031.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trade</th>\n",
       "      <td>592</td>\n",
       "      <td>430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agreement</th>\n",
       "      <td>156</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income-based</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>straits</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hot-spots</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>face-coverings</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thatll</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24086 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                freq  neg_freq\n",
       "word                          \n",
       "detroit           17      12.0\n",
       "--              1385    1063.0\n",
       "new             1314    1031.0\n",
       "trade            592     430.0\n",
       "agreement        156      84.0\n",
       "...              ...       ...\n",
       "income-based       1       NaN\n",
       "straits            1       NaN\n",
       "hot-spots          1       NaN\n",
       "face-coverings     1       NaN\n",
       "thatll             1       NaN\n",
       "\n",
       "[24086 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df2.groupby('date').apply(lambda x: x.sample(frac=0.1))\n",
    "train_ind = [index[1] for index in train_lag.index.tolist()]\n",
    "\n",
    "df2['data'] = 'test'\n",
    "df2.loc[train_ind, 'data'] = 'train'\n",
    "\n",
    "# for training dataset\n",
    "pos_word = create_df(1, 'train', df2)\n",
    "neg_word = create_df(-1, 'train', df2)\n",
    "\n",
    "neg_word.columns = ['word', 'neg_freq']\n",
    "# pos_word.columns = ['word', 'neg_freq']\n",
    "word = pos_word.set_index('word').join(neg_word.set_index('word')) # join pos_word, neg_word dataframe\n",
    "\n",
    "# word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56ccd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wolag = filter_df(word, 50).sort_values(by = ['pos_prob'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2ea01a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>neg_freq</th>\n",
       "      <th>total_freq</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>neg_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17308</th>\n",
       "      <td>cocoa</td>\n",
       "      <td>78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>0.012658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3866</th>\n",
       "      <td>measles</td>\n",
       "      <td>81</td>\n",
       "      <td>13.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.861702</td>\n",
       "      <td>0.138298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>mcdonalds</td>\n",
       "      <td>59</td>\n",
       "      <td>12.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.169014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>children</td>\n",
       "      <td>113</td>\n",
       "      <td>27.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.192857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4113</th>\n",
       "      <td>senate</td>\n",
       "      <td>45</td>\n",
       "      <td>11.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.196429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  freq  neg_freq  total_freq  pos_prob  neg_prob\n",
       "17308      cocoa    78       1.0        79.0  0.987342  0.012658\n",
       "3866     measles    81      13.0        94.0  0.861702  0.138298\n",
       "1043   mcdonalds    59      12.0        71.0  0.830986  0.169014\n",
       "2608    children   113      27.0       140.0  0.807143  0.192857\n",
       "4113      senate    45      11.0        56.0  0.803571  0.196429"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wolag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4978718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence interval of positive seed words:  (0.5337788246494162, 0.5790208965598802)\n",
      "confidence interval of negative seed words:  (0.42097910344011996, 0.4662211753505839)\n"
     ]
    }
   ],
   "source": [
    "########### to be confirmed #############\n",
    "import statsmodels.stats.proportion as smp\n",
    "\n",
    "thres = 0.555\n",
    "pos = df_wolag[df_wolag['pos_prob'] >= thres]\n",
    "count = len(pos)\n",
    "num = len(df_prob)\n",
    "\n",
    "print('confidence interval of positive seed words: ', smp.proportion_confint (count, num, alpha=0.05, method='wilson'))\n",
    "print('confidence interval of negative seed words: ', smp.proportion_confint (num - count, num, alpha=0.05, method='wilson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1dec73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### to be confirmed #############\n",
    "df_wolag['polar'] = 'positive'\n",
    "df_wolag.loc[df_wolag[df_wolag['pos_prob'] < 0.555].index, 'polar'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6a464f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wolag.to_csv('wsj_seed_word.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48096a60",
   "metadata": {},
   "source": [
    "## 4. Embedding\n",
    "\n",
    "two possible ways to reduce the dimension of the embeddings before sentprop:\n",
    "1. PCA https://towardsdatascience.com/dimension-reduction-techniques-with-python-f36ca7009e5c\n",
    "2. t-SNE https://arxiv.org/abs/1708.03629; https://github.com/vyraun/Half-Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f124a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages\n",
    "import gensim.downloader as api\n",
    "import tempfile\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d03193ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preparation\n",
    "cleaned_cors = pd.read_csv('cleaned_corpus.csv') # import the cleaned dataframe\n",
    "\n",
    "## Change Numbers info for placeholder keep signs \n",
    "txt_new = []\n",
    "reg = re.compile(r\"([\\\\+\\\\-])?[0-9]+[0-9\\\\.]*\")\n",
    "for lines in cleaned_cors[\"Text\"].values:\n",
    "    txt_new.append(reg.sub(\" \\\\1NUM\", lines))\n",
    "\n",
    "## Define punctuation to replace (Exclude +, -, and %)\n",
    "new_punct = string.punctuation + \"“”’\"\n",
    "for symb in [\"%\", \"+\", \"-\", \"&\"]:\n",
    "    new_punct = new_punct.replace(symb, \"\")\n",
    "\n",
    "## String list\n",
    "txt_corp = []\n",
    "for doc in txt_new:\n",
    "    ## Change everything to lowercase and exclude string that are only punctuations\n",
    "    aux = [elem.lower() for elem in doc.split() if elem not in set(new_punct)]\n",
    "    txt_corp.append(aux)\n",
    "\n",
    "## Remove strings that only have punctuation signs\n",
    "exclude = [\"\"]\n",
    "txt_end = []\n",
    "for doc in txt_corp:\n",
    "    new_list = [elem.translate(str.maketrans('', '', new_punct)) for elem in doc]\n",
    "    txt_end.append([elem for elem in new_list if elem not in exclude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7bf7194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = corpora.Dictionary(txt_end)\n",
    "\n",
    "## Define function to get embeddings to memory\n",
    "def get_wv(model, dicts):\n",
    "    \"\"\" Get word embeddings in memory\"\"\"\n",
    "    w2v_embed = {}\n",
    "    missing = []\n",
    "    for val in dicts.values():\n",
    "        try: \n",
    "            it = model.wv[val]\n",
    "        except:\n",
    "            missing.append(val)\n",
    "            it = None \n",
    "        w2v_embed[val] = it\n",
    "    return w2v_embed, missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d335469b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words:  116106\n",
      "number of unique words after fitlering:  17429\n"
     ]
    }
   ],
   "source": [
    "print('number of unique words: ', len(dicts))\n",
    "\n",
    "dicts.filter_extremes(no_below=20, no_above=0.8, keep_n=None, keep_tokens=None)\n",
    "print('number of unique words after fitlering: ', len(dicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42361bd4",
   "metadata": {},
   "source": [
    "### 4.1 pre-trained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "88609b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "model = Word2Vec(txt_corp, size = 300, min_count = 25)\n",
    "model.intersect_word2vec_format(path,\n",
    "                                lockf=1.0,\n",
    "                                binary=True)\n",
    "\n",
    "model.train(txt_corp, total_examples=model.corpus_count, epochs=25)\n",
    "w2v_embed, mis  = get_wv(model, dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d5174c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_1df = pd.DataFrame(w2v_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e6bd3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_1df.to_csv('pre_embedding.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f1b33",
   "metadata": {},
   "source": [
    "### 4.2 Self-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "471a7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = Word2Vec(txt_corp, window=5, min_count=25, workers=4, size = 50)\n",
    "model_t.train(txt_corp, epochs=50, total_words = model_t.corpus_total_words,\n",
    "              total_examples = model_t.corpus_count)\n",
    "embeds_2 = get_wv(model_t, dicts)\n",
    "a, b = embeds_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5b51946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_2df = pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8830aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embedding to csv\n",
    "embeds_2df.to_csv('self_embedding.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample_env",
   "language": "python",
   "name": "sample_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
